{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "_fMRAv5JIyus"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMET\n",
        "Commonsense Transformers for Automatic Knowledge Graph Construction \n",
        "- [ACL 2019 Paper](https://arxiv.org/abs/1906.05317)\n"
      ],
      "metadata": {
        "id": "jw0KgBeba1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# references = {Bosselut2019COMETCT,\n",
        "#   title={COMET: Commonsense Transformers for Automatic Knowledge Graph Construction},\n",
        "#   author={Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Ã‡elikyilmaz and Yejin Choi},\n",
        "#   booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},\n",
        "#   year={2019}}\n",
        "import os \n",
        "import urllib \n",
        "!git clone https://github.com/atcbosselut/comet-commonsense.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoYoc5dnkj3i",
        "outputId": "1ccd7572-6f4e-4d66-e1ff-fb1ef0b577aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'comet-commonsense' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Data Load\n",
        "- Data: [WN18RR](https://github.com/JD-AI-Research-Silicon-Valley/SACN)\n",
        "    - contains 93,003 triples with 40,943 entities and 11 relation types\n",
        "    - link prediction dataset created from WN18, a subset of WordNet\n",
        "        - limit of WN18: test leakage \n",
        "            - many text triples are obtained by inverting triples from the training set \n",
        "        - WN18RR: created to ensure that the evaluation dataset does not have inverse relation test leakage \n",
        "- Data Implementation References: \n",
        "    - [WN18RR impelementation](https://github.com/villmow/datasets_knowledge_embedding/tree/master/WN18RR/text)\n",
        "    - [create_WN18RR](https://github.com/JD-AI-Research-Silicon-Valley/SACN/tree/master/data)\n",
        "    - [WN18](https://github.com/uclnlp/inferbeddings)"
      ],
      "metadata": {
        "id": "_fMRAv5JIyus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# the paths do not exist (we do not want to encourage the usage of WN18) and the filenames are off, but if you put in the WN18 files\n",
        "# and adjust the path you will generate WN18RR.\n",
        "\n",
        "predicates_to_remove = [\n",
        "    '_member_of_domain_topic',\n",
        "    '_synset_domain_usage_of',\n",
        "    '_instance_hyponym',\n",
        "    '_hyponym',\n",
        "    '_member_holonym',\n",
        "    '_synset_domain_region_of',  \n",
        "    '_part_of'\n",
        "]   \n",
        "\n",
        "def read_triples(path):     \n",
        "    triples = []\n",
        "    with open(path, 'rt') as f:\n",
        "        for line in f.readlines():\n",
        "            s, p, o = line.split('\\t')\n",
        "            triples += [(s.strip(), p.strip(), o.strip())]\n",
        "    return triples\n",
        "\n",
        "def write_triples(triples, path):\n",
        "    with open(path, 'wt') as f:\n",
        "        for (s, p, o) in triples:\n",
        "            f.write('{}\\t{}\\t{}\\n'.format(s, p, o))\n",
        "\n",
        "train_triples = read_triples('original/wordnet-mlj12-train.txt') \n",
        "valid_triples = read_triples('original/wordnet-mlj12-valid.txt')\n",
        "test_triples = read_triples('original/wordnet-mlj12-test.txt')   \n",
        "\n",
        "filtered_train_triples = [(s, p, o) for (s, p, o) in train_triples if p not in predicates_to_remove]\n",
        "filtered_valid_triples = [(s, p, o) for (s, p, o) in valid_triples if p not in predicates_to_remove]\n",
        "filtered_test_triples = [(s, p, o) for (s, p, o) in test_triples if p not in predicates_to_remove]\n",
        "\n",
        "write_triples(filtered_train_triples, 'wn18-train.tsv')\n",
        "write_triples(filtered_valid_triples, 'wn18-valid.tsv')\n",
        "write_triples(filtered_test_triples, 'wn18-test.tsv')     "
      ],
      "metadata": {
        "id": "aT1ZQgaDbzpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Model Architecture\n",
        "1. Problem: Difference between conventional KBs and commonsense knowledge \n",
        "    - traditional KB: fixed structured format (two entities with a known relation) \n",
        "    - commonsense knowledge: open-text description of knowledge (loosely structured)\n",
        "\n",
        "2. COMET: automatically generate Commonsense Knowledge base graph model \n",
        "    - Experiment: ATOMIC, ConcepNet \n",
        "    - *BUT here in this implementation code, Experiment: WN18RR*\n",
        "\n",
        "3. Model Architecture: \n",
        "    - basic structure is similar with GPT (Radford et al. 2018) \n",
        "    - composed of Transformer Blocks, which contains Multi-head Attention block and Feed-Forward Network \n",
        "4. Pre-train Transformer: a seed set of knowledge \n",
        "    - during pre-training: \n",
        "        - representation revision for knowledge generation \n",
        "    - result: generating high-level new tuples (=commonsense knowledge) \n"
      ],
      "metadata": {
        "id": "Md35UGpZbz0N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gu0Rrwvnb0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CzANHU9b018"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Training\n",
        "- import src.data.config as cfg\n",
        "- import src.data.data as data\n",
        "- import src.train.utils as train_utils\n",
        "- import src.train.batch as batch\n",
        "\n",
        "- import src.evaluate.evaluate as evaluate\n",
        "- import src.evaluate.generate as gen\n",
        "- import src.evaluate.sampler as sampling\n",
        "\n",
        "- import utils.utils as utils"
      ],
      "metadata": {
        "id": "mGbqsZAkb019"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 train.py\n",
        "[src/train/train.py](https://github.com/atcbosselut/comet-commonsense/blob/master/src/train/train.py)"
      ],
      "metadata": {
        "id": "9E2953S6Sz8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "# !pip install tensorboard\n",
        "import tensorboard\n",
        "import torchvision   \n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms       "
      ],
      "metadata": {
        "id": "Qrzwbvqfb01_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, opt, meta, data_loader, model, optimizer):\n",
        "        self.optimizer = optimizer \n",
        "        self.model = model \n",
        "\n",
        "        if opt.trainer == 'epoch':\n",
        "            self.epochs = meta.epochs \n",
        "        self.data_loader = data_loader\n",
        "        self.opt = opt \n",
        "\n",
        "        self.losses = {'dev':{}, 'test':{}, 'train':{}}\n",
        "        self.top_score = None\n",
        "        self.lrs = {}\n",
        "\n",
        "        self.batch_variables = {\n",
        "            'data': self.data_loader,\n",
        "            'model': self.model,\n",
        "            'split': 'train'}\n",
        "\n",
        "        self.do_gen = cfg.do_gen\n",
        "        self.samplers = {}\n",
        "\n",
        "    def decide_to_save(self):\n",
        "        to_save = cfg.save and not cfg.toy \n",
        "        to_save = to_save or cfg.test_save \n",
        "        print(cfg.save_strategy)\n",
        "        if cfg.save_strategy == 'best':\n",
        "            if self.top_score[0] != self.opt.tarin.dynamic.epoch:\n",
        "                print('DOING IT RIGHT')\n",
        "                to_save = False \n",
        "        return to_save \n",
        "    \n",
        "    def save_model(self, tracked_score):\n",
        "        lrs = {}\n",
        "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
        "            lrs[i] = param_group['lr']\n",
        "        self.lrs[self.opt.tarin.dynamic.epoch] = lrs \n",
        "        to_save = self.decide_to_save()\n",
        "\n",
        "        if to_save:\n",
        "            data.save_step(\n",
        "                self.model, self.data_loader.vocab_encoder,\n",
        "                self.optimizer, self.opt, \n",
        "                self.opt.train.dynamic.epoch, self.lrs)\n",
        "            \n",
        "    def log_losses(self, opt, losses):  \n",
        "        if (not cfg.toy and cfg.save) or cfg.test_save: \n",
        "            data.save_eval_file(opt, losses['train'], 'losses', split='train')\n",
        "            data.save_eval_file(opt, losses['dev'], 'losses',split='dev')   \n",
        "\n",
        "    def set_logger(self): \n",
        "        if cfg.toy: \n",
        "            self.logger = SummaryWriter(utils.make_name(\n",
        "                self.opt, prefix='garbage/logs/', eval_=True, do_epoch=False))\n",
        "        else: \n",
        "            self.logger = SummaryWriter(utils.make_name(\n",
        "                self.opt, prefix='logs/', eval_=True, do_epoch=False))\n",
        "        print(f'Logging Tensorboard Files at: {self.logger.logdir}')\n",
        "\n",
        "    def stop_logger(self):\n",
        "        self.logger.close()\n",
        "\n",
        "    def run(self): \n",
        "        self.set_logger()\n",
        "        self.count = 0\n",
        "        for epoch in range(self.epochs): \n",
        "            self.model.train()\n",
        "            self.opt.train.dynamic.epoch += 1\n",
        "            self.epoch()\n",
        "        self.stop_logger()\n",
        "\n",
        "    def epoch(self): \n",
        "        nums = self.reset_losses()\n",
        "\n",
        "        # initialize progress bar \n",
        "        bar = utils.initialize_progress_bar(\n",
        "            self.data_loader.sequences['train'])\n",
        "        reset = False \n",
        "        \n",
        "        while not reset: \n",
        "            loss, nums, reset = self.do_forward_pass(nums)\n",
        "            self.do_backward_pass(loss)\n",
        "            self.update_parameters()\n",
        "\n",
        "            bar.update(self.opt.train.dynamic.bs)\n",
        "            self.count += 1\n",
        "\n",
        "            for loss_name in self.losses['train']:\n",
        "                self.logger.add_scalar(\n",
        "                    f'train/{loss_name}',  \n",
        "                    loss.item() / self.opt.tarin.dynamic.bs,\n",
        "                    self.count)\n",
        "                \n",
        "            if cfg.toy and self.counter(nums) > 300:\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.run_evaluation_cycle()\n",
        "\n",
        "\n",
        "            self.log_losses(self.opt, self.losses)\n",
        "            self.update_top_score(self.opt)\n",
        "            self.save_model(self.get_tracked_score())\n",
        "            self.data_loader.reset_offsets('train')\n",
        "\n",
        "    def run_evaluation_cycle(self):\n",
        "        for split in ['dev','test']:\n",
        "            self.evaluator.validate(\n",
        "                self.opt.train.dynamic.epoch, \n",
        "                split,\n",
        "                self.losses[split])\n",
        "            \n",
        "            if self.do_gen: \n",
        "                gen.do_gen_run(\n",
        "                    self.opt, self.generator, self.opt.train.dynamic.epoch,\n",
        "                    split,\n",
        "                    self.losses[split])\n",
        "            iter_num = self.opt.train.dynamic.epoch \n",
        "\n",
        "            for loss_name in self.losses[split]:\n",
        "                self.logger.add_scalar(\n",
        "                    f'{split}/{loss_name}',\n",
        "                    self.losses[split][loss_name][iter_num],\n",
        "                    iter_num)\n",
        "                \n",
        "    def clip_gradients(self):\n",
        "        if self.opt.train.static.clip:\n",
        "            torch.nn.utils.clip_grad_norm(\n",
        "                self.model.parameters(), self.opt.train.static.clip)\n",
        "            \n",
        "    def do_forward_pass(self, nums): \n",
        "        token_loss, nums, reset = self.batch(\n",
        "            self.opt, nums, self.losses['train'],\n",
        "            self.batch_variables)\n",
        "        return token_loss, nums, reset \n",
        "\n",
        "    def do_backward_pass(self, loss): \n",
        "        loss.backward()\n",
        "\n",
        "    def update_parameters(self):   \n",
        "        if self.opt.model == 'lstm': \n",
        "                self.clip_graients()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "    \n",
        "    def reset_losses(self): \n",
        "        loss_names = set([i.rstrip('maicro').rstrip('_') for \n",
        "                            i in self.losses['train'].keys()])\n",
        "        return self.initialize_losses(list(loss_names))   \n"
      ],
      "metadata": {
        "id": "mxx2NHplb02A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EYcOZSN2nFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSHVDFG_b02B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPc7sQt4b1Cv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgvA1Kgib1Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XapK_YlQb1Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-3XIuZfb1Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bhc11Fdb1Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vm7czKmVb1Cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}